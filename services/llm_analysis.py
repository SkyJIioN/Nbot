import os
import requests

def generate_analysis_prompt(symbol, timeframe, rsi, sma, ema, macd, macd_signal, current_price, volume):
    return f"""
üìä –ê–Ω–∞–ª—ñ–∑ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∏ {symbol} –Ω–∞ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ñ {timeframe.upper()}:

–ü–æ—Ç–æ—á–Ω–∞ —Ü—ñ–Ω–∞: {current_price:.2f}$
–û–± º—î–º —Ç–æ—Ä–≥—ñ–≤: {volume}

–Ü–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏:
- RSI: {rsi:.2f}
- SMA: {sma:.2f}
- EMA: {ema:.2f}
- MACD: {macd:.2f}
- MACD Signal: {macd_signal:.2f}

–ù–∞ –æ—Å–Ω–æ–≤—ñ —Ü–∏—Ö –¥–∞–Ω–∏—Ö:
1. –í–∫–∞–∂–∏ —Ç–æ—Ä–≥–æ–≤—É —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—é: LONG, SHORT –∞–±–æ WAIT
2. –í–∫–∞–∂–∏ —Ç–æ—á–∫—É –≤—Ö–æ–¥—É –π —Ü—ñ–ª—å –¥–ª—è –≤–∏—Ö–æ–¥—É
3. –ö–æ—Ä–æ—Ç–∫–æ –ø–æ—è—Å–Ω–∏, —á–æ–º—É —Å–∞–º–µ —Ç–∞–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è
"""

def get_llm_recommendation(symbol, timeframe, rsi, sma, ema, macd, macd_signal, current_price, volume):
    prompt = generate_analysis_prompt(symbol, timeframe, rsi, sma, ema, macd, macd_signal, current_price, volume)

    headers = {
        "Authorization": f"Bearer {os.getenv('GROQ_API_KEY')}",
        "Content-Type": "application/json"
    }

    body = {
        "model": "llama3-70b-8192",
        "messages": [
            {"role": "system", "content": "–¢–∏ –¥–æ—Å–≤—ñ–¥—á–µ–Ω–∏–π —Ç—Ä–µ–π–¥–µ—Ä-–∞–Ω–∞–ª—ñ—Ç–∏–∫, —è–∫–∏–π –∫–æ—Ä–æ—Ç–∫–æ —ñ —á—ñ—Ç–∫–æ –¥–∞—î —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7
    }

    response = requests.post("https://api.groq.com/openai/v1/chat/completions", headers=headers, json=body)

    if response.status_code == 200:
        return response.json()["choices"][0]["message"]["content"]
    else:
        print("LLM Error:", response.text)
        return "‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ –®–Ü."
